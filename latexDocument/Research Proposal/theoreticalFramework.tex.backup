\section{Theoretical framework}
Reinforcement Learning is an area in machine learning in which an agent needs to take actions in an environment which provides the highest reward for that agent. The environment of a reinforcement learning problem
is described as a Markov decision process (MDP) \cite{markov}. A MDP consists out of States, Actions, Rewards, and a Transition that maps how from a current State, by performing an Action, an agent gets into a new State. In this project
we will use continues actions instead of discrete actions. The goal is to find a policy that will provide the best action based on the current state. 
For this purpose we will use the Continues Actor Critic Learning Automaton (CACLA) algorithm \cite{van2007reinforcement}. CACLA is created to handle the continues action space, which is needed to control a robotic arm. An action in this case is the velocity of a single actuator (joint) of the 
robotic arm, which can move with different speeds. CACLA is a model-free algorithm, meaning that the agent does not need know about the environment beforehand, it can therefore be used in all sorts of problems, like controlling a 
robotic arm. An important part of Reinforcement Learning is exploration, this enables the algorithm to find different policies, and from this the best policy should be found. 
The CACLA paper suggests two exploration methods. The first one is the $\epsilon$-greedy method, it will select a random action with $\epsilon$ probability. The second method is Gaussian exploration,
this method will always explore, but takes the actual action output as a mean and will add some Gaussian noise to that action. \\
To improve learning extra parameters will be added to the CACLA algorithm to test their performance. For example we can look at the insights from the Deep Q Network (DQN) \cite{mnih2015human}. DQN shows for example that it is possible for 
large, non-linear networks to become robust and stable by training networks off-policy by using samples from a replay buffer to minimize the correlations between samples. In the 'Continues control with deep reinforcement learning' paper
\cite{lillicrap2015continuous}, that also uses a actor-critic network for learning continues control, they make use of 
batch normalization [REF], a recent advance in deep learning. Also methods like dropout [REF] to reduce overfitting will be tested. \\
For the perception part, object localization, we will make use of CNNs. CNNs don't learn features directly from the input image but they learn kernels that each can represent a feature. CNN are successfully used in 
object recognition [REF] and speech recognition [REF]. In this project we want to use CNNs to determine the location of an object that the arm can grasp. [REF] show that they can determine the 3D pose of a hand by using 
a depth image and CNNs.

 
 %- explain RL *
 %- explain CACLA *
 % - Talk about exploration, reward functions, robotics grasping/control
 % Won't use grasping planners, want to investigate RL on simple grasping. 
 %- parameters *
 %- explain CNN
 % 3d CNN used for 3d hand pose, use for object location. 
